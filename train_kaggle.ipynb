{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12075019,"sourceType":"datasetVersion","datasetId":7600920},{"sourceId":12100189,"sourceType":"datasetVersion","datasetId":7617734}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:37:56.246699Z","iopub.execute_input":"2025-06-11T17:37:56.247397Z","iopub.status.idle":"2025-06-11T17:37:56.368347Z","shell.execute_reply.started":"2025-06-11T17:37:56.247369Z","shell.execute_reply":"2025-06-11T17:37:56.367190Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!rm -rf /kaggle/working/vilegaljere/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T08:46:53.897948Z","iopub.execute_input":"2025-06-18T08:46:53.898242Z","iopub.status.idle":"2025-06-18T08:46:54.148757Z","shell.execute_reply.started":"2025-06-18T08:46:53.898212Z","shell.execute_reply":"2025-06-18T08:46:54.147941Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/Sonny-Inkai/KTLN.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T08:46:56.338796Z","iopub.execute_input":"2025-06-18T08:46:56.339570Z","iopub.status.idle":"2025-06-18T08:46:57.789987Z","shell.execute_reply.started":"2025-06-18T08:46:56.339540Z","shell.execute_reply":"2025-06-18T08:46:57.789312Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'KTLN'...\nremote: Enumerating objects: 5327, done.\u001b[K\nremote: Counting objects: 100% (5327/5327), done.\u001b[K\nremote: Compressing objects: 100% (3555/3555), done.\u001b[K\nremote: Total 5327 (delta 1731), reused 5326 (delta 1730), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (5327/5327), 11.57 MiB | 22.18 MiB/s, done.\nResolving deltas: 100% (1731/1731), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T08:46:58.773384Z","iopub.execute_input":"2025-06-18T08:46:58.773895Z","iopub.status.idle":"2025-06-18T08:46:58.893058Z","shell.execute_reply.started":"2025-06-18T08:46:58.773866Z","shell.execute_reply":"2025-06-18T08:46:58.892420Z"}},"outputs":[{"name":"stdout","text":"KTLN  out_vilegal_t5small\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%cd KTLN/VilegalJERE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T08:47:23.298004Z","iopub.execute_input":"2025-06-18T08:47:23.298330Z","iopub.status.idle":"2025-06-18T08:47:23.305141Z","shell.execute_reply.started":"2025-06-18T08:47:23.298302Z","shell.execute_reply":"2025-06-18T08:47:23.304303Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/KTLN/VilegalJERE\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T08:47:24.570660Z","iopub.execute_input":"2025-06-18T08:47:24.571184Z","iopub.status.idle":"2025-06-18T08:47:24.689969Z","shell.execute_reply.started":"2025-06-18T08:47:24.571162Z","shell.execute_reply":"2025-06-18T08:47:24.689108Z"}},"outputs":[{"name":"stdout","text":"configurator.py  LICENSE  pretrain.sh\t    tokenizer\t\t   wandb\ngenerate.py\t model\t  requirements.txt  train_vilegal_jere.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!torchrun --standalone --nproc_per_node=2 train_vilegal_jere.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T08:47:38.682183Z","iopub.execute_input":"2025-06-18T08:47:38.682539Z","execution_failed":"2025-06-18T09:32:05.774Z"}},"outputs":[{"name":"stdout","text":"W0618 08:47:43.047000 88 torch/distributed/run.py:792] \nW0618 08:47:43.047000 88 torch/distributed/run.py:792] *****************************************\nW0618 08:47:43.047000 88 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0618 08:47:43.047000 88 torch/distributed/run.py:792] *****************************************\n[W618 08:47:53.039137978 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W618 08:48:03.049551998 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n2025-06-18 08:48:14.844129: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-18 08:48:14.844131: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750236495.059424      92 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750236495.059414      93 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750236495.118908      93 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1750236495.118918      92 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\ntokenizer_config.json: 100%|███████████████| 23.7k/23.7k [00:00<00:00, 85.0MB/s]\nspiece.model: 100%|██████████████████████████| 389k/389k [00:00<00:00, 10.6MB/s]\ntokenizer.json: 100%|██████████████████████| 1.38M/1.38M [00:00<00:00, 26.0MB/s]\nspecial_tokens_map.json: 100%|█████████████| 2.67k/2.67k [00:00<00:00, 18.7MB/s]\nYou set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\nYou set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nWORLD_SIZE: 2, RANK: 1, LOCAL_RANK: 1\nWORLD_SIZE: 2, RANK: 0, LOCAL_RANK: 0\n[W618 08:48:39.971567324 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W618 08:48:39.023677444 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W618 08:48:49.977874384 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W618 08:48:59.988438229 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\nRunning in 'Pre-training' mode.\nLoading Vietnamese legal data for pre-training...\nLoading Vietnamese legal data for pre-training...\nLoaded 9373 legal articles\nTrain data size: 8435, Val data size: 938\n🔧 TOKENIZER DEBUG INFO:\n  Vocab size: 10100\n  Pad token: '<pad>' (id: 0)\n  EOS token: '</s>' (id: 3)\n  UNK token: '<unk>' (id: 1)\n  Sentinel <extra_id_0>: 10099\n🎯 MODEL CONFIG:\n  decoder_start_token_id: 3 (should match EOS)\nInitializing a new model from scratch\nLoaded 9373 legal articles\nTrain data size: 8435, Val data size: 938\n🔧 TOKENIZER DEBUG INFO:\n  Vocab size: 10100\n  Pad token: '<pad>' (id: 0)\n  EOS token: '</s>' (id: 3)\n  UNK token: '<unk>' (id: 1)\n  Sentinel <extra_id_0>: 10099\n🎯 MODEL CONFIG:\n  decoder_start_token_id: 3 (should match EOS)\nModel initialized with 50.1M parameters\nModel initialized with 50.1M parameters\nStarting Pre-training ViLegalJERE with 50.1M parameters...\nTraining data size: 8435, Val data size: 938\nBatch size: 32, Gradient accumulation: 4\nEffective batch size: 256\nMode: Pre-training, Learning rate: 0.0003, Max iters: 10000\nFirst batch shapes - Input: torch.Size([32, 512]), Decoder: torch.Size([32, 512]), Labels: torch.Size([32, 512])\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mngomonone123\u001b[0m (\u001b[33mngomonone123-university-information-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n✅ Đăng nhập wandb thành công!\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/KTLN/VilegalJERE/wandb/run-20250618_084908-agsn2d5o\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mViLegal_50m_T5small_Kaggle_20250618_084829\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ngomonone123-university-information-of-technology/ViLegalJERE-T5Small\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ngomonone123-university-information-of-technology/ViLegalJERE-T5Small/runs/agsn2d5o\u001b[0m\nStarting Pre-training ViLegalJERE with 50.1M parameters...\nTraining data size: 8435, Val data size: 938\nBatch size: 32, Gradient accumulation: 4\nEffective batch size: 256\nMode: Pre-training, Learning rate: 0.0003, Max iters: 10000\nFirst batch shapes - Input: torch.Size([32, 512]), Decoder: torch.Size([32, 512]), Labels: torch.Size([32, 512])\niter 0: loss 9.3738, time 4494.47ms, mfu -100.00%, tps (M) 0.06, tokens trained 262144.00B\niter 10: loss 9.2502, time 3852.15ms, mfu 917.39%, tps (M) 0.07, tokens trained 2883584.00B\niter 20: loss 8.9862, time 3938.26ms, mfu 915.39%, tps (M) 0.07, tokens trained 5505024.00B\niter 30: loss 8.7132, time 4059.26ms, mfu 910.91%, tps (M) 0.06, tokens trained 8126464.00B\niter 50: loss 8.3836, time 4141.01ms, mfu 899.84%, tps (M) 0.06, tokens trained 13369344.00B\niter 60: loss 8.2653, time 4154.27ms, mfu 894.93%, tps (M) 0.06, tokens trained 15990784.00B\niter 70: loss 8.2557, time 4148.33ms, mfu 890.62%, tps (M) 0.06, tokens trained 18612224.00B\niter 80: loss 8.0755, time 4142.81ms, mfu 886.86%, tps (M) 0.06, tokens trained 21233664.00B\niter 90: loss 8.0724, time 4174.77ms, mfu 882.83%, tps (M) 0.06, tokens trained 23855104.00B\niter 100: loss 7.8372, time 4157.23ms, mfu 879.55%, tps (M) 0.06, tokens trained 26476544.00B\niter 110: loss 7.8238, time 4148.78ms, mfu 876.78%, tps (M) 0.06, tokens trained 29097984.00B\niter 130: loss 7.4772, time 4148.33ms, mfu 871.92%, tps (M) 0.06, tokens trained 34340864.00B\niter 140: loss 7.2862, time 4145.13ms, mfu 869.98%, tps (M) 0.06, tokens trained 36962304.00B\niter 150: loss 7.1890, time 4158.02ms, mfu 867.98%, tps (M) 0.06, tokens trained 39583744.00B\niter 160: loss 6.9974, time 4151.27ms, mfu 866.31%, tps (M) 0.06, tokens trained 42205184.00B\niter 170: loss 6.8491, time 4143.11ms, mfu 864.97%, tps (M) 0.06, tokens trained 44826624.00B\niter 180: loss 6.7291, time 4150.18ms, mfu 863.63%, tps (M) 0.06, tokens trained 47448064.00B\niter 190: loss 6.6588, time 4141.33ms, mfu 862.60%, tps (M) 0.06, tokens trained 50069504.00B\niter 200: loss 6.5262, time 4151.72ms, mfu 861.46%, tps (M) 0.06, tokens trained 52690944.00B\niter 210: loss 6.4237, time 4144.72ms, mfu 860.58%, tps (M) 0.06, tokens trained 55312384.00B\niter 220: loss 6.4628, time 4148.32ms, mfu 859.71%, tps (M) 0.06, tokens trained 57933824.00B\niter 230: loss 6.5040, time 4155.94ms, mfu 858.77%, tps (M) 0.06, tokens trained 60555264.00B\niter 240: loss 6.3950, time 4166.43ms, mfu 857.71%, tps (M) 0.06, tokens trained 63176704.00B\niter 250: loss 6.3378, time 4157.08ms, mfu 856.95%, tps (M) 0.06, tokens trained 65798144.00B\niter 260: loss 6.2954, time 4157.31ms, mfu 856.26%, tps (M) 0.06, tokens trained 68419584.00B\niter 270: loss 6.2571, time 4150.08ms, mfu 855.79%, tps (M) 0.06, tokens trained 71041024.00B\niter 280: loss 6.3212, time 4159.23ms, mfu 855.18%, tps (M) 0.06, tokens trained 73662464.00B\niter 300: loss 6.3702, time 4145.54ms, mfu 854.72%, tps (M) 0.06, tokens trained 78905344.00B\niter 310: loss 6.2909, time 4144.14ms, mfu 854.52%, tps (M) 0.06, tokens trained 81526784.00B\niter 320: loss 6.2574, time 4147.41ms, mfu 854.28%, tps (M) 0.06, tokens trained 84148224.00B\niter 330: loss 6.2352, time 4156.29ms, mfu 853.88%, tps (M) 0.06, tokens trained 86769664.00B\niter 340: loss 6.2434, time 4159.18ms, mfu 853.46%, tps (M) 0.06, tokens trained 89391104.00B\niter 350: loss 6.3343, time 4153.76ms, mfu 853.19%, tps (M) 0.06, tokens trained 92012544.00B\niter 360: loss 6.3258, time 4134.21ms, mfu 853.35%, tps (M) 0.06, tokens trained 94633984.00B\niter 380: loss 6.2300, time 4151.85ms, mfu 853.21%, tps (M) 0.06, tokens trained 99876864.00B\niter 390: loss 6.2703, time 4165.97ms, mfu 852.71%, tps (M) 0.06, tokens trained 102498304.00B\niter 400: loss 6.3024, time 4156.30ms, mfu 852.47%, tps (M) 0.06, tokens trained 105119744.00B\niter 410: loss 6.2789, time 4138.87ms, mfu 852.61%, tps (M) 0.06, tokens trained 107741184.00B\niter 420: loss 6.2583, time 4158.23ms, mfu 852.33%, tps (M) 0.06, tokens trained 110362624.00B\niter 430: loss 6.2476, time 4149.11ms, mfu 852.27%, tps (M) 0.06, tokens trained 112984064.00B\niter 440: loss 6.2100, time 4150.70ms, mfu 852.19%, tps (M) 0.06, tokens trained 115605504.00B\niter 450: loss 6.1602, time 4160.15ms, mfu 851.91%, tps (M) 0.06, tokens trained 118226944.00B\niter 460: loss 6.2645, time 4162.07ms, mfu 851.63%, tps (M) 0.06, tokens trained 120848384.00B\niter 470: loss 6.1470, time 4161.28ms, mfu 851.39%, tps (M) 0.06, tokens trained 123469824.00B\niter 480: loss 6.1640, time 4162.45ms, mfu 851.15%, tps (M) 0.06, tokens trained 126091264.00B\niter 490: loss 6.2005, time 4148.44ms, mfu 851.22%, tps (M) 0.06, tokens trained 128712704.00B\nstep 500: train loss 6.1542, val loss 6.1099\nSaving checkpoint to /kaggle/working/out_vilegal_t5small\niter 500: loss 6.2649, time 152519.47ms, mfu 768.42%, tps (M) 0.00, tokens trained 131334144.00B\niter 510: loss 6.2128, time 4147.33ms, mfu 776.79%, tps (M) 0.06, tokens trained 133955584.00B\niter 520: loss 6.2170, time 4150.68ms, mfu 784.25%, tps (M) 0.06, tokens trained 136577024.00B\niter 530: loss 6.1216, time 4151.71ms, mfu 790.94%, tps (M) 0.06, tokens trained 139198464.00B\niter 540: loss 6.0312, time 4159.79ms, mfu 796.80%, tps (M) 0.06, tokens trained 141819904.00B\niter 550: loss 6.0954, time 4147.60ms, mfu 802.33%, tps (M) 0.06, tokens trained 144441344.00B\niter 560: loss 6.1670, time 4178.31ms, mfu 806.67%, tps (M) 0.06, tokens trained 147062784.00B\niter 570: loss 6.2750, time 4155.84ms, mfu 811.04%, tps (M) 0.06, tokens trained 149684224.00B\niter 580: loss 6.0815, time 4164.49ms, mfu 814.80%, tps (M) 0.06, tokens trained 152305664.00B\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}